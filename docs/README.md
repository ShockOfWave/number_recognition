# Документация по разработке решения для задачи распознавания чисел с помощью CRNN

Этот документ описывает ход рассуждений, эксперименты и принятые решения при разработке решения для распознавания чисел по изображениям с использованием CRNN (Convolutional Recurrent Neural Network) на базе PyTorch и PyTorch Lightning.

## 1. Постановка задачи

В задаче требуется:
- Распознавание цены (числовое значение) с изображений.
- Обеспечение точного извлечения последовательности цифр.
- Оценка модели с использованием метрик:
  - **На уровне изображения:** точность (accuracy), precision, recall, F1-score для правильного распознавания всего числа.
  - **На уровне цифр:** precision, recall, F1-score для отдельных цифр.
- Предоставление возможности инференса через командную строку, FastAPI-эндпоинт и Docker.

## 2. Выбор метода и архитектуры

### 2.1. Первоначальные идеи и эксперименты

- **OCR vs. Обучаемая модель:**  
  В начале можно было попробовать использовать готовые OCR-решения, такие как pytesseract и EasyOCR. Однако они не показали высокой эффективности. Для их работы потребовалось бы доразмечать изображения, возможно обучать AE или VAE, чтобы восстанавливать качество, т.к. они плохо распознают при имеющемся качестве. Также необходимо было бы учить `object detection` модель на keypoints, чтобы выравнивать цифры горизонтально. В связи с этим, вариант был исключен.

- **CNN классификация и регрессия**  
  Для классификации слишком большой размах, 10000 классов. Регрессия же мало подходит для получения точного числа. Пробовал брать backbone типа ResNet и дообучать его на регрессию с округлением, но такой подход не имел успеха.

- **CRNN с CTC Loss:**  
  Для распознавания последовательностей символов (в нашем случае цифр) было решено использовать CRNN в связке с функцией потерь CTC (Connectionist Temporal Classification). Это позволяет:
  - Не требовать точного выравнивания символов с выходными временными шагами.
  - Работать с переменной длиной последовательностей (например, цена может состоять из разного числа цифр).
  Идею взял из этой статьи на [habr](https://habr.com/ru/articles/535224/)

### 2.2. Использование PyTorch Lightning

- **Удобство и модульность:**  
  PyTorch Lightning упрощает организацию кода для обучения, валидации и тестирования модели, позволяя сконцентрироваться на самой модели и логике шага обучения.
- **Логирование и коллбэки:**  
  Lightning позволяет легко интегрировать логгеры (например, wandb), а также использовать коллбэки для ранней остановки (Early Stopping), сохранения лучших чекпоинтов (ModelCheckpoint) и планировщиков обучения (например, ReduceLROnPlateau).

## 3. Архитектура решения

### 3.1. Модель

#### 3.1.1. Сверточный блок

Сверточный блок состоит из нескольких последовательных слоев:
- **Сверточные слои с увеличением числа фильтров:** Мы увеличили мощность модели, изменив число фильтров (например, 128 → 256 → 512 → 1024). Это позволяет модели извлекать более сложные признаки.
- **Пулинг и BatchNorm:** Используются для уменьшения размерности и стабилизации обучения.
- **Dropout:** Добавлен для регуляризации, чтобы бороться с переобучением.

#### 3.1.2. Рекуррентный блок

Для обработки последовательности признаков, полученной после сверточного блока, используется Bidirectional LSTM. Он обрабатывает данные в обеих направлениях, что помогает лучше захватывать контекст цифр в последовательности.

#### 3.1.3. Функция потерь

Используется CTC Loss, позволяющая обучать модель без явного выравнивания между выходными временными шагами и целевой последовательностью цифр.

### 3.2. Декодирование с помощью Beam Search

Для преобразования выходов модели (логарифмы вероятностей по классам) в итоговую последовательность цифр используется **beam search декодирование**:
- **Beam Search:** На каждом временном шаге рассматриваются несколько кандидатов (профилей), а затем отбирается лучший с точки зрения суммарной логарифмической вероятности. Это позволяет улучшить качество предсказаний по сравнению с жадным (greedy) декодированием, особенно в сложных случаях.
- **Логарифмическая сумма (log_sum_exp):** Для устойчивости вычислений используется функция log_sum_exp, которая корректно суммирует логарифмы вероятностей.
- **Параметры:** Ширина луча (beam width) настраивается (например, 10), что позволяет находить более оптимальное декодирование последовательности.

### 3.3. Расстояние Левенштейна

Для оценки качества распознавания отдельных цифр используется **расстояние Левенштейна**:
- **Определение:** Это метрика, измеряющая минимальное количество операций (вставка, удаление, замена), необходимых для преобразования одной строки в другую.
- **Использование:** На основе расстояния Левенштейна вычисляются precision, recall и F1-score для распознавания цифр. Если модель пропустила или ошибочно добавила цифру, это отражается в увеличении расстояния, что позволяет объективно оценивать качество распознавания.

### 3.2. Структура проекта

Решил разбить проект на несколько модулей для повышения удобства поддержки и масштабирования. Основные компоненты расположены в папке `src` с подпапками:

- `src/data` – содержит датасеты и DataModule для обучения, валидации и теста.
- `src/models` – содержит реализацию модели (CRNN, Bidirectional LSTM) и LightningModule.
- `src/inference` – содержит класс для инференса (`CRNNInference`), который можно использовать как отдельно, так и интегрировать в API.
- `src/utils` – содержит вспомогательные функции, например, для beam search декодирования и расчёта метрик (расстояние Левенштейна).

Также в корне проекта находятся скрипты:
- `train.py` – для обучения модели (с параметрами, логированием, коллбэками, early stopping, scheduler).
- `inference.py` – (опционально) для запуска инференса через командную строку.
- `app.py` – для FastAPI-эндпоинта.

## 4. Ход разработки и принятые решения

### 4.1. Выбор и доработка архитектуры

- **Начало:**  
  Изначально рассматривались решения на базе OCR (например, pytesseract), но по результатам экспериментов было решено, что собственная обучаемая модель дает больше контроля над качеством распознавания.
  
- **Эксперименты с CRNN:**  
  Были реализованы базовая CRNN-архитектура и функция декодирования с использованием beam search. Проверял, как модель справляется с различными размерами чисел и как меняется качество распознавания при изменении гиперпараметров.

- **Увеличение мощности модели:**  
  После получения предварительных результатов модель показала высокую точность на обучающем наборе, но наблюдался оверфит. Для борьбы с этим были добавлены:
  - Увеличение аугментации (RandomHorizontalFlip, RandomRotation).
  - Dropout в сверточном блоке.
  - Регуляризация (weight decay в оптимизаторе).
  - Планировщик обучения (ReduceLROnPlateau) и ранняя остановка (EarlyStopping).

### 4.2. Возможности улучшения
1. Нужно оптимизировать выход с backbone, т.к. количество каналов на последнем слое необходимо подгонять под максимальную длину последовательности. В целом, количество параметров было выбрано сильно с запасом, т.к. не было мощностей и времени проводить полноценную оптимизацию.
2. Можно дообучить какой-нибудь AE или VAE, чтобы они восстанавливали качество данных (в случае VAE можно еще погенерить данные). На после восстановления явно можно повысить точность. Вставить это как отдельную часть пайплайна.

### 4.2. Организация кода

- **Модульность:**  
  Разделение на пакеты (`data`, `models`, `inference`, `utils`) позволяет легко поддерживать и расширять проект. Например, если потребуется изменить стратегию инференса, достаточно будет изменить файл `src/inference/inference.py`.
  
- **Использование PyTorch Lightning:**  
  Lightning упрощает написание циклов обучения и логирование метрик, что особенно полезно при работе с такими метриками, как точность на уровне изображения и цифр. Это позволило более гибко настроить обучение и интегрировать логгеры (например, WandB).

- **Инференс и API:**  
  Был создан класс `CRNNInference`, который инкапсулирует всю логику предсказания, что позволяет использовать его не только в командной строке, но и в качестве компонента для FastAPI-эндпоинта.

### 4.3. Интеграция Docker и линтинг

- **Docker:**  
  Для облегчения развертывания и воспроизводимости сборка проекта была оформлена через Docker. Это позволяет запускать приложение в изолированном окружении, где установлены все зависимости.

- **Линтинг и проверка кода:**  
  В проект добавлены конфигурационные файлы для flake8, black, isort и mypy, что позволяет поддерживать высокий уровень качества кода.

## 5. Итог

Разработка решения проходила в несколько этапов:
1. **Анализ задачи** – изучение требований, выбор CRNN для распознавания последовательностей цифр.
2. **Реализация базовой модели** – построение архитектуры с использованием сверточных и рекуррентных слоёв, применение CTC Loss.
3. **Модернизация и регуляризация** – увеличение мощности модели, добавление аугментации, Dropout, weight decay, ReduceLROnPlateau, EarlyStopping.
4. **Организация проекта** – разделение кода на модули для удобства поддержки и расширения.
5. **Инференс и API** – создание класса для инференса и FastAPI-эндпоинта для интеграции в продакшн.
6. **Документация и CI/CD** – настройка линтеров и Docker для обеспечения качества кода и воспроизводимости.

## 6. Заключение

Этот проект демонстрирует подход к решению задачи распознавания числовых значений с использованием современной архитектуры CRNN и инструментов PyTorch Lightning. Благодаря модульной организации, поддержке инференса через API и Docker-контейнер, наше решение легко масштабируется и интегрируется в производственные системы.

Если у вас возникнут вопросы или предложения по улучшению пишите.

---
